---
title: "Estimating a Bayes factor using the Savage-Dickey density ratio"
description: | 
  For a simple mean comparison
date: 05-29-2021
categories:
  - multilevel models
  - Bayes factor
author:
  - first_name: "Andrew"
    last_name: "Ellis"
    url: https://github.com/awellis
    affiliation: Cognitive psychology, perception & methods, Univerity of Bern 
    affiliation_url: https://www.kog.psy.unibe.ch
    orcid_id: 0000-0002-2788-936X

citation_url: https://awellis.github.io/learnmultilevelmodels/bayes-factor-group-difference.html
bibliography: ../../bibliography.bib
output: 
    distill::distill_article:
      toc: true
      toc_float: true
      toc_depth: 2
      code_folding: false
      css: ../../css/style.css
---





```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(brms)
library(tidybayes)

theme_set(theme_grey(base_size = 14) +
            theme(panel.grid = element_blank()))
```


# Bayes factor for difference in means

We are going to look at data from a study in which children with ADHD were compared to a control group in terms of their working memory capacity


```{r}
library(tidyverse)
nback <- read_csv("https://raw.githubusercontent.com/kogpsy/neuroscicomplab/main/data/adhd-nback.csv")
```

Die grouping variable `group` should be converted to a factor.


```{r}
nback <- nback %>% 
  mutate(group = as_factor(group),
         group = fct_relevel(group, "control"))
```

A directed Welch test result in a p-value of approx. 0.07. We cannot reject the null hypothesis.

```{r}
t.test(rt ~ group,
       data = nback,
       alternative = "less")
```

We now want to perform a Bayesian model comparison using a Bayes factor, in order to determine whether there is evidence that the ADHD group has a larger mean than the control group. However, we will start by estimating a Bayes factor for the undirected hypotheses that the group means differ.


:::exercise
**Exercise 1**
Estimate an intercept and a group difference. Use a `normal(0, 1)` for the group difference.
:::

```{r echo=FALSE, include=FALSE}
library(brms)

get_prior(rt ~ 1 + group,
          data = nback)
priors <- prior(normal(0, 1), class = b)
```

```{r echo=FALSE, include=FALSE}
m1 <- brm(rt ~ group,
          prior = priors,
          data = nback,
          file = "models/exc4-m1")
```


```{r}
summary(m1)
```

The population level effects `Intercept` and `groudadhd` represent the expected mean of the control group, and the expected difference between groups. The parameter has a mean of  $0.16$ a  $95%$ credible interval of  $[-0.04, 0.35]$.

You can use the `hypothesis()` function to get the number of samples that are positive divided by the number of samples that are negative.


:::exercise
**Exercise 2**

a) Use the `hypothesis()` function to obtain the probability that the parameter is $>0$.

b) Save the output of `hypothesis()` and plot it using `plot()`.
:::


```{r}
h1 <- m1 %>% 
  hypothesis("groupadhd > 0")

h1
```

This returns the ratio of positive versus negative samples from the posterior distribution. THe evidence ratio `Evid.Ratio` represents the area under the posterior greater than zero divided by the area under the posterior less than zero. In this case the evidence ratio is $17.18$, which means that positive values are $17.18$ more probable than negative values, or the positive area is $17.18$ times greater than the negative area. 

We can also compute the posterior probability that the value is positive. This means that we have to find the probability $p_+$ for which $\frac{p_+}{1-p_+} = 17.18$. This results in $p_+ = 17.18/(1 + 17.18) = 0.945$, which is given in the output under `Post.Prob`.



```{r}
p_pos <- 17.18 / (1 + 17.18)
p_pos
```



```{r}
plot(h1)
```

:::exercise
**Exercise 3**

Refit the model from exercise 1, but this time adding the argument `sample_prior = TRUE`, and saving as`m2`.
:::



## Two-sided 

```{r}
m2 <- brm(rt ~ group,
          prior = priors,
          sample_prior = TRUE,
          data = nback1,
          file = "models/exc4-m2")
```

By including this argument we are telling `brms` to sample from both the prior and posterior distributions, which we need to compute the Savage-Dickey density ratio.

We can plot both prior and posterior, i.e. what we believed about our parameter before seeing the data, and our updated belief after taking the data into account.

```{r}
m2 %>% 
  mcmc_plot(c("b_groupadhd", "prior_b"))
```



:::exercise
**Exercise 4**

Try to wuantify evidence for the null hypothesis that the group difference is zero, using the 
 `hypothesis()` function to compute the Savage-Dickey density ratio. 


- save the ouput as `h2`.
- inspect the ouput using `print(h2)` or simply `h2`.
- the evidence ratio `Evid.Ratio` is the Bayes factor $BF_{01}$. THe ouput of `hypothesis()` is a list;`Evid.Ratio` can be accessed under `h2$hypothesis$Evid.Ratio`. Save this as `BF01`.
- Show the Bayes factor $BF_{10}$ (this is easy if you already have $BF_{01}$). What does $BF_{10}$ tell us?
:::


```{r}
h2 <- m2 %>% 
  hypothesis("groupadhd = 0")
h2
```

```{r}
BF01 <- h2$hypothesis$Evid.Ratio
BF01
```

The Bayes factor $BF_{01}$ tells us how much more likely the data are under the null hypothesis than under the alternative hypothesis.. The alternative hypothesis corresponds to our prior on the group difference, which stated that we were 95% certain that the difference lies between -2 and 2. This prior is not very specific, and the resulting Bayes factor reflects this---under the null hypothesis, the data are approximately 2.6 times more likely than under the alternative hypothesis. Despite the fact that when estimating the parameter we are almost 95% certain that the parameter is positive, the Bayes factor provides evidence for the null hypothesis. The Bayes factor for the alternative hypothesis is $0.38$.



```{r}
BF10 <- 1/BF01
BF10
```

:::note
We have to extremely careful when chooing prior distributions when estimating Bayes factors. Uninformative priors are useful when estimating parameters, but are a bad idea when estimating Bayes factors.
:::



:::exercise
**Exercise 5**

Plot the output `hypothesis()` using the `plot()` method.
:::


```{r echo=FALSE, include=FALSE}
plot(h2)
```

The density evaluated at 0 is higher under the posterior than under prior, leading to the BF for the null.



## One-sided

Now we will attempt to estimate the Bayes factor $BF_{10}$ for the directed hypothesis that the mean of the ADHD group is greater than that of the control group.

This hypothesis states that the group difference is constraine to be positive. Therefore, we will use a lower bound on our prior distribution. Furthermore, we expect the difference to be small (based on previous research), so we use a small standar deviation of $0.1$. Combining both of these beliefs, we can express our prior as  `prior(normal(0, 0.1), lb = 0)`. This corresponds to a folded normal distribution, or a half-normal.

```{r}
tibble(x = seq(-0.5, 1, by = 0.01),
       y = if_else(x >= 0,  dnorm(x, 0, 0.1), 0)) |> 
  ggplot(aes(x, abs(y))) +
  geom_line(size = 1.5) +
  xlab("") + ylab("") +
  ggtitle("Half-normal distribution")
```




```{r}
m3 <- brm(rt ~ group,
          prior = prior(normal(0, 0.1), lb = 0),
          sample_prior = TRUE,
          data = nback1,
          file = "models/exc4-m3")
```


We plot both prior and posterior, as before. THe prior now has zero density for values $<0$.



```{r}
m3 %>% 
  mcmc_plot(c("b_groupadhd", "prior_b"))
```

Now we can use the same hypothesis as before---we are now comparing the null to an alternative that states that the group difference is small, but positive.

```{r}
h3 <- m3 %>% 
  hypothesis("groupadhd = 0")
h3
```

The Bayes factor is now $0.42$ in favour of the null hypothesis. This means that the data are $0.42$ times as likely under the null. 

```{r}
BF01alt <- h3$hypothesis$Evid.Ratio
BF01alt
```

This is better expresssed as a Bayes factor in favour of the alternative, $BF_{10}$.

```{r}
BF10alt <- 1/BF01alt
BF10alt
```

Now the data are $2.36$ more likely to have occurred under the alternative than under the null hypothesis.

:::note
The Bayes factor tells us how well a model's prior distributions predicted the data. The Bayes factor is this related to the prior predictive distribution. Our hypotheses correspond to prior distributions on the parameters, and thus it is extremely important that our priors reflect our hypotheses.
:::

