
@article{baayenMixedeffectsModelingCrossed2008,
  title = {Mixed-Effects Modeling with Crossed Random Effects for Subjects and Items},
  author = {Baayen, R. H. and Davidson, D. J. and Bates, D. M.},
  date = {2008-11-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {59},
  pages = {390--412},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2007.12.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X07001398},
  urldate = {2021-05-29},
  abstract = {This paper provides an introduction to mixed-effects models for the analysis of repeated measurement data with subjects and items as crossed random effects. A worked-out example of how to use recent software for mixed-effects modeling is provided. Simulation studies illustrate the advantages offered by mixed-effects analyses compared to traditional analyses based on quasi-F tests, by-subjects analyses, combined by-subjects and by-items analyses, and random regression. Applications and possibilities across a range of domains of inquiry are discussed.},
  file = {/Users/andrew/Dropbox/Zotero/Baayen et al/2008/Baayen et al. - 2008 - Mixed-effects modeling with crossed random effects.pdf;/Users/andrew/Zotero/storage/APG6V6YZ/S0749596X07001398.html},
  keywords = {By-item,By-subject,Crossed random effects,Mixed-effects models,Quasi-F},
  langid = {english},
  number = {4},
  series = {Special {{Issue}}: {{Emerging Data Analysis}}}
}

@article{batesFittingLinearMixedEffects2015,
  title = {Fitting {{Linear Mixed}}-{{Effects Models Using}} {\textbf{Lme4}}},
  author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
  date = {2015},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {67},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  url = {http://www.jstatsoft.org/v67/i01/},
  urldate = {2021-05-29},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  file = {/Users/andrew/Dropbox/Zotero/Bates et al/2015/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf},
  langid = {english},
  number = {1}
}

@article{burknerAdvancedBayesianMultilevel2018,
  title = {Advanced {{Bayesian Multilevel Modeling}} with the {{R Package}} Brms},
  author = {Bürkner, Paul-Christian},
  date = {2018},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  volume = {10},
  pages = {395},
  issn = {2073-4859},
  doi = {10.32614/RJ-2018-017},
  url = {https://journal.r-project.org/archive/2018/RJ-2018-017/index.html},
  urldate = {2021-05-29},
  abstract = {The brms package allows R users to easily specify a wide range of Bayesian single-level and multilevel models which are fit with the probabilistic programming language Stan behind the scenes. Several response distributions are supported, of which all parameters (e.g., location, scale, and shape) can be predicted. Non-linear relationships may be specified using non-linear predictor terms or semi-parametric approaches such as splines or Gaussian processes. Multivariate models can be fit as well. To make all of these modeling options possible in a multilevel framework, brms provides an intuitive and powerful formula syntax, which extends the well known formula syntax of lme4. The purpose of the present paper is to introduce this syntax in detail and to demonstrate its usefulness with four examples, each showing relevant aspects of the syntax.},
  file = {/Users/andrew/Zotero/storage/E6RJQ5AS/Bürkner - 2018 - Advanced Bayesian Multilevel Modeling with the R P.pdf},
  langid = {english},
  number = {1}
}

@online{burknerBayesianItemResponse2020,
  title = {Bayesian {{Item Response Modeling}} in {{R}} with Brms and {{Stan}}},
  author = {Bürkner, Paul-Christian},
  date = {2020-02-01},
  url = {http://arxiv.org/abs/1905.09501},
  urldate = {2021-04-26},
  abstract = {Item Response Theory (IRT) is widely applied in the human sciences to model persons’ responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective prespecified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. I demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and postprocessed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.},
  archiveprefix = {arXiv},
  eprint = {1905.09501},
  eprinttype = {arxiv},
  file = {/Users/andrew/Dropbox/Zotero/Bürkner/2020/Bürkner - 2020 - Bayesian Item Response Modeling in R with brms and.pdf},
  keywords = {Statistics - Computation},
  langid = {english},
  primaryclass = {stat}
}

@article{gelmanRsquaredBayesianRegression2019,
  title = {R-Squared for {{Bayesian Regression Models}}},
  author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Vehtari, Aki},
  date = {2019-07-03},
  journaltitle = {The American Statistician},
  volume = {73},
  pages = {307--309},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1549100},
  url = {https://doi.org/10.1080/00031305.2018.1549100},
  urldate = {2021-05-28},
  abstract = {The usual definition of R2 (variance of the predicted values divided by the variance of the data) has a problem for Bayesian fits, as the numerator can be larger than the denominator. We propose an alternative definition similar to one that has appeared in the survival analysis literature: the variance of the predicted values divided by the variance of predicted values plus the expected variance of the errors.},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1549100},
  file = {/Users/andrew/Dropbox/Zotero/Gelman et al/2019/Gelman et al. - 2019 - R-squared for Bayesian Regression Models.pdf;/Users/andrew/Zotero/storage/MRCS9Q3Y/00031305.2018.html},
  keywords = {Bayesian methods,R-squared,Regression},
  number = {3}
}

@article{gigerenzerMindlessStatistics2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  date = {2004-11},
  journaltitle = {The Journal of Socio-Economics},
  volume = {33},
  pages = {587--606},
  issn = {10535357},
  doi = {10.1016/j.socec.2004.09.033},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053535704000927},
  urldate = {2019-02-11},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  file = {/Users/andrew/Dropbox/Zotero/Gigerenzer2004/Gigerenzer - 2004 - Mindless statistics.pdf},
  langid = {english},
  number = {5}
}

@article{gigerenzerStatisticalRitualsReplication2018a,
  title = {Statistical {{Rituals}}: {{The Replication Delusion}} and {{How We Got There}}},
  shorttitle = {Statistical {{Rituals}}},
  author = {Gigerenzer, Gerd},
  date = {2018-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {198--218},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918771329},
  url = {https://doi.org/10.1177/2515245918771329},
  urldate = {2021-03-01},
  abstract = {The “replication crisis” has been attributed to misguided external incentives gamed by researchers (the strategic-game hypothesis). Here, I want to draw attention to a complementary internal factor, namely, researchers’ widespread faith in a statistical ritual and associated delusions (the statistical-ritual hypothesis). The “null ritual,” unknown in statistics proper, eliminates judgment precisely at points where statistical theories demand it. The crucial delusion is that the p value specifies the probability of a successful replication (i.e., 1 – p), which makes replication studies appear to be superfluous. A review of studies with 839 academic psychologists and 991 students shows that the replication delusion existed among 20\% of the faculty teaching statistics in psychology, 39\% of the professors and lecturers, and 66\% of the students. Two further beliefs, the illusion of certainty (e.g., that statistical significance proves that an effect exists) and Bayesian wishful thinking (e.g., that the probability of the alternative hypothesis being true is 1 – p), also make successful replication appear to be certain or almost certain, respectively. In every study reviewed, the majority of researchers (56\%–97\%) exhibited one or more of these delusions. Psychology departments need to begin teaching statistical thinking, not rituals, and journal editors should no longer accept manuscripts that report results as “significant” or “not significant.”},
  file = {/Users/andrew/Dropbox/Zotero/Gigerenzer/2018/Gigerenzer - 2018 - Statistical Rituals The Replication Delusion and .pdf},
  keywords = {illusion of certainty,null ritual,p value,p-hacking,replication},
  langid = {english},
  number = {2}
}

@article{gronauTutorialBridgeSampling2017a,
  title = {A Tutorial on Bridge Sampling},
  author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric-Jan and Steingroever, Helen},
  date = {2017-12-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {81},
  pages = {80--97},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2017.09.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249617300640},
  urldate = {2021-05-03},
  abstract = {The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng \& Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model—a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.},
  file = {/Users/andrew/Dropbox/Zotero/Gronau et al/2017/Gronau et al. - 2017 - A tutorial on bridge sampling.pdf;/Users/andrew/Zotero/storage/8MGSE8Q5/S0022249617300640.html},
  keywords = {Bayes factor,Hierarchical model,Marginal likelihood,Normalizing constant,Predictive accuracy,Reinforcement learning},
  langid = {english}
}

@book{kruschkeDoingBayesianData2015,
  title = {Doing Bayesian Data Analysis (Second Edition)},
  author = {Kruschke, John},
  date = {2015},
  publisher = {{Academic Press}},
  location = {{Boston}},
  added-at = {2016-12-29T09:25:25.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/277f97f6f84d077790b702e30ed86be5f/becker},
  interhash = {5e4043e24f7f58a8de076d7956ca08ea},
  intrahash = {77f97f6f84d077790b702e30ed86be5f},
  keywords = {diss imported inthesis mixedtrails},
  timestamp = {2017-06-19T10:12:05.000+0200}
}

@article{limpertLognormalDistributionsSciences2001,
  title = {Log-Normal {{Distributions}} across the {{Sciences}}: {{Keys}} and {{Clues}}: {{On}} the Charms of Statistics, and How Mechanical Models Resembling Gambling Machines Offer a Link to a Handy Way to Characterize Log-Normal Distributions, Which Can Provide Deeper Insight into Variability and Probability—Normal or Log-Normal: {{That}} Is the Question},
  shorttitle = {Log-Normal {{Distributions}} across the {{Sciences}}},
  author = {Limpert, Eckhard and Stahel, Werner A. and Abbt, Markus},
  date = {2001-05-01},
  journaltitle = {BioScience},
  shortjournal = {BioScience},
  volume = {51},
  pages = {341--352},
  issn = {0006-3568},
  doi = {10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2},
  url = {https://doi.org/10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2},
  urldate = {2021-05-13},
  file = {/Users/andrew/Dropbox/Zotero/Limpert et al/2001/Limpert et al. - 2001 - Log-normal Distributions across the Sciences Keys.pdf;/Users/andrew/Zotero/storage/PE4KDHDX/243981.html},
  number = {5}
}

@book{mcelreathStatisticalRethinkingBayesian2020a,
  title = {Statistical Rethinking: {{A}} Bayesian Course with Examples in {{R}} and {{Stan}}, 2nd Edition},
  author = {McElreath, Richard},
  date = {2020},
  edition = {2},
  publisher = {{CRC Press}},
  url = {http://xcelab.net/rm/statistical-rethinking/}
}

@article{moscatelliModelingPsychophysicalData2012a,
  title = {Modeling Psychophysical Data at the Population-Level: {{The}} Generalized Linear Mixed Model},
  shorttitle = {Modeling Psychophysical Data at the Population-Level},
  author = {Moscatelli, A. and Mezzetti, M. and Lacquaniti, F.},
  date = {2012-10-25},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {12},
  pages = {26--26},
  issn = {1534-7362},
  doi = {10.1167/12.11.26},
  url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/12.11.26},
  urldate = {2021-04-26},
  abstract = {In psychophysics, researchers usually apply a two-level model for the analysis of the behavior of the single subject and the population. This classical model has two main disadvantages. First, the second level of the analysis discards information on trial repetitions and subject-specific variability. Second, the model does not easily allow assessing the goodness of fit. As an alternative to this classical approach, here we propose the Generalized Linear Mixed Model (GLMM). The GLMM separately estimates the variability of fixed and random effects, it has a higher statistical power, and it allows an easier assessment of the goodness of fit compared with the classical two-level model. GLMMs have been frequently used in many disciplines since the 1990s; however, they have been rarely applied in psychophysics. Furthermore, to our knowledge, the issue of estimating the point-of-subjective-equivalence (PSE) within the GLMM framework has never been addressed. Therefore the article has two purposes: It provides a brief introduction to the usage of the GLMM in psychophysics, and it evaluates two different methods to estimate the PSE and its variability within the GLMM framework. We compare the performance of the GLMM and the classical two-level model on published experimental data and simulated data. We report that the estimated values of the parameters were similar between the two models and Type I errors were below the confidence level in both models. However, the GLMM has a higher statistical power than the two-level model. Moreover, one can easily compare the fit of different GLMMs according to different criteria. In conclusion, we argue that the GLMM can be a useful method in psychophysics.},
  file = {/Users/andrew/Dropbox/Zotero/Moscatelli et al/2012/Moscatelli et al. - 2012 - Modeling psychophysical data at the population-lev.pdf},
  langid = {english},
  number = {11}
}

@online{MultilevelAnalysisTechniques,
  title = {Multilevel {{Analysis}}: {{Techniques}} and {{Applications}}, {{Third Edition}}},
  shorttitle = {Multilevel {{Analysis}}},
  url = {https://www.routledge.com/Multilevel-Analysis-Techniques-and-Applications-Third-Edition/Hox-Moerbeek-Schoot/p/book/9781138121362},
  urldate = {2021-05-29},
  abstract = {Applauded for its clarity, this accessible introduction helps readers apply multilevel techniques to their research. The book also includes advanced extensions, making it useful as both an introduction for students and as a reference for researchers. Basic models and examples are discussed in nontechnical terms with an emphasis on understanding the methodological and statistical issues involved in using these models. The estimation and interpretation of multilevel models is demonstrated using re},
  file = {/Users/andrew/Zotero/storage/D2HVYDIF/9781138121362.html},
  langid = {english},
  organization = {{Routledge \& CRC Press}}
}

@article{schadHowCapitalizePriori2020,
  title = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: {{A}} Tutorial},
  shorttitle = {How to Capitalize on a Priori Contrasts in Linear (Mixed) Models},
  author = {Schad, Daniel J. and Vasishth, Shravan and Hohenstein, Sven and Kliegl, Reinhold},
  date = {2020-02-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {110},
  pages = {104038},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2019.104038},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X19300695},
  urldate = {2021-04-23},
  abstract = {Factorial experiments in research on memory, language, and in other areas are often analyzed using analysis of variance (ANOVA). However, for effects with more than one numerator degrees of freedom, e.g., for experimental factors with more than two levels, the ANOVA omnibus F-test is not informative about the source of a main effect or interaction. Because researchers typically have specific hypotheses about which condition means differ from each other, a priori contrasts (i.e., comparisons planned before the sample means are known) between specific conditions or combinations of conditions are the appropriate way to represent such hypotheses in the statistical model. Many researchers have pointed out that contrasts should be “tested instead of, rather than as a supplement to, the ordinary ‘omnibus’ F test” (Hays, 1973, p. 601). In this tutorial, we explain the mathematics underlying different kinds of contrasts (i.e., treatment, sum, repeated, polynomial, custom, nested, interaction contrasts), discuss their properties, and demonstrate how they are applied in the R System for Statistical Computing (R Core Team, 2018). In this context, we explain the generalized inverse which is needed to compute the coefficients for contrasts that test hypotheses that are not covered by the default set of contrasts. A detailed understanding of contrast coding is crucial for successful and correct specification in linear models (including linear mixed models). Contrasts defined a priori yield far more useful confirmatory tests of experimental hypotheses than standard omnibus F-tests. Reproducible code is available from https://osf.io/7ukf6/.},
  file = {/Users/andrew/Dropbox/Zotero/Schad et al/2020/Schad et al. - 2020 - How to capitalize on a priori contrasts in linear .pdf;/Users/andrew/Zotero/storage/K95QEX9J/S0749596X19300695.html},
  keywords = {A priori hypotheses,Contrasts,Linear models,Null hypothesis significance testing},
  langid = {english}
}

@online{schadPrincipledBayesianWorkflow2019,
  title = {Toward a Principled {{Bayesian}} Workflow in Cognitive Science},
  author = {Schad, Daniel J. and Betancourt, Michael and Vasishth, Shravan},
  date = {2019-04-29},
  url = {http://arxiv.org/abs/1904.12765},
  urldate = {2019-04-30},
  abstract = {Experiments in research on memory, language, and in other areas of cognitive science are increasingly being analyzed using Bayesian methods. This has been facilitated by the development of probabilistic programming languages such as Stan, and easily accessible front-end packages such as brms. However, the utility of Bayesian methods ultimately depends on the relevance of the Bayesian model, in particular whether or not it accurately captures the structure of the data and the data analyst's domain expertise. Even with powerful software, the analyst is responsible for verifying the utility of their model. To accomplish this, we introduce a principled Bayesian workflow (Betancourt, 2018) to cognitive science. Using a concrete working example, we describe basic questions one should ask about the model: prior predictive checks, computational faithfulness, model sensitivity, and posterior predictive checks. The running example for demonstrating the workflow is data on reading times with a linguistic manipulation of object versus subject relative sentences. This principled Bayesian workflow also demonstrates how to use domain knowledge to inform prior distributions. It provides guidelines and checks for valid data analysis, avoiding overfitting complex models to noise, and capturing relevant data structure in a probabilistic model. Given the increasing use of Bayesian methods, we aim to discuss how these methods can be properly employed to obtain robust answers to scientific questions.},
  archiveprefix = {arXiv},
  eprint = {1904.12765},
  eprinttype = {arxiv},
  file = {/Users/andrew/Dropbox/Zotero/Schad et al2019/Schad et al. - 2019 - Toward a principled Bayesian workflow in cognitive.pdf;/Users/andrew/Zotero/storage/3K5CU9B6/1904.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@online{schadWorkflowTechniquesRobust2021,
  title = {Workflow {{Techniques}} for the {{Robust Use}} of {{Bayes Factors}}},
  author = {Schad, Daniel J. and Nicenboim, Bruno and Bürkner, Paul-Christian and Betancourt, Michael and Vasishth, Shravan},
  date = {2021-03-15},
  url = {http://arxiv.org/abs/2103.08744},
  urldate = {2021-03-20},
  abstract = {Inferences about hypotheses are ubiquitous in the cognitive sciences. Bayes factors provide one general way to compare different hypotheses by their compatibility with the observed data. Those quantifications can then also be used to choose between hypotheses. While Bayes factors provide an immediate approach to hypothesis testing, they are highly sensitive to details of the data/model assumptions. Moreover it's not clear how straightforwardly this approach can be implemented in practice, and in particular how sensitive it is to the details of the computational implementation. Here, we investigate these questions for Bayes factor analyses in the cognitive sciences. We explain the statistics underlying Bayes factors as a tool for Bayesian inferences and discuss that utility functions are needed for principled decisions on hypotheses. Next, we study how Bayes factors misbehave under different conditions. This includes a study of errors in the estimation of Bayes factors. Importantly, it is unknown whether Bayes factor estimates based on bridge sampling are unbiased for complex analyses. We are the first to use simulation-based calibration as a tool to test the accuracy of Bayes factor estimates. Moreover, we study how stable Bayes factors are against different MCMC draws. We moreover study how Bayes factors depend on variation in the data. We also look at variability of decisions based on Bayes factors and how to optimize decisions using a utility function. We outline a Bayes factor workflow that researchers can use to study whether Bayes factors are robust for their individual analysis, and we illustrate this workflow using an example from the cognitive sciences. We hope that this study will provide a workflow to test the strengths and limitations of Bayes factors as a way to quantify evidence in support of scientific hypotheses. Reproducible code is available from https://osf.io/y354c/.},
  archiveprefix = {arXiv},
  eprint = {2103.08744},
  eprinttype = {arxiv},
  file = {/Users/andrew/Dropbox/Zotero/Schad et al/2021/Schad et al. - 2021 - Workflow Techniques for the Robust Use of Bayes Fa.pdf;/Users/andrew/Zotero/storage/ICBKF5S8/2103.html},
  keywords = {Statistics - Methodology},
  primaryclass = {stat}
}

@book{shravanvasishthIntroductionBayesianData,
  title = {An {{Introduction}} to {{Bayesian Data Analysis}} for {{Cognitive Science}}},
  author = {{Shravan Vasishth} and {Bruno Nicenboim} and {Daniel Schad}},
  url = {https://vasishth.github.io/Bayes_CogSci/},
  urldate = {2021-05-29},
  abstract = {An introduction to Bayesian data analysis for Cognitive Science.},
  file = {/Users/andrew/Zotero/storage/S2D7U8M2/book.html}
}

@incollection{singmannIntroductionMixedModels2019,
  title = {An {{Introduction}} to {{Mixed Models}} for {{Experimental Psychology}}},
  booktitle = {New {{Methods}} in {{Cognitive Psychology}}},
  author = {Singmann, Henrik and Kellen, David},
  editor = {Spieler, Daniel and Schumacher, Eric},
  date = {2019-10-28},
  edition = {1},
  pages = {4--31},
  publisher = {{Routledge}},
  doi = {10.4324/9780429318405-2},
  url = {https://www.taylorfrancis.com/books/9781000617467/chapters/10.4324/9780429318405-2},
  urldate = {2021-02-18},
  file = {/Users/andrew/Zotero/storage/SBC6A7LI/Singmann and Kellen - 2019 - An Introduction to Mixed Models for Experimental P.pdf},
  isbn = {978-0-429-31840-5},
  langid = {english}
}

@article{vandeschootBayesianStatisticsModelling2021,
  title = {Bayesian Statistics and Modelling},
  author = {van de Schoot, Rens and Depaoli, Sarah and King, Ruth and Kramer, Bianca and Märtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher},
  date = {2021-12},
  journaltitle = {Nature Reviews Methods Primers},
  shortjournal = {Nat Rev Methods Primers},
  volume = {1},
  pages = {1},
  issn = {2662-8449},
  doi = {10.1038/s43586-020-00001-2},
  url = {http://www.nature.com/articles/s43586-020-00001-2},
  urldate = {2021-04-16},
  abstract = {Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.},
  file = {/Users/andrew/Dropbox/Zotero/van de Schoot et al/2021/van de Schoot et al. - 2021 - Bayesian statistics and modelling.pdf},
  langid = {english},
  number = {1},
  options = {useprefix=true}
}

@article{wagenmakersLinearRelationMean2007,
  title = {On the Linear Relation between the Mean and the Standard Deviation of a Response Time Distribution.},
  author = {Wagenmakers, Eric-Jan and Brown, Scott},
  date = {2007},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  volume = {114},
  pages = {830--841},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.114.3.830},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.114.3.830},
  urldate = {2021-05-10},
  abstract = {Although it is generally accepted that the spread of a response time (RT) distribution increases with the mean, the precise nature of this relation remains relatively unexplored. The authors show that in several descriptive RT distributions, the standard deviation increases linearly with the mean. Results from a wide range of tasks from different experimental paradigms support a linear relation between RT mean and RT standard deviation. Both R. Ratcliff’s (1978) diffusion model and G. D. Logan’s (1988) instance theory of automatization provide explanations for this linear relation. The authors identify and discuss 3 specific boundary conditions for the linear law to hold. The law constrains RT models and supports the use of the coefficient of variation to (a) compare variability while controlling for differences in baseline speed of processing and (b) assess whether changes in performance with practice are due to quantitative speedup or qualitative reorganization.},
  file = {/Users/andrew/Dropbox/Zotero/Wagenmakers_Brown/2007/Wagenmakers and Brown - 2007 - On the linear relation between the mean and the st.pdf},
  langid = {english},
  number = {3}
}

@article{wassersteinASAStatementPValues2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  date = {2016-04-02},
  journaltitle = {The American Statistician},
  volume = {70},
  pages = {129--133},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2016.1154108},
  url = {https://doi.org/10.1080/00031305.2016.1154108},
  urldate = {2021-03-01},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2016.1154108},
  file = {/Users/andrew/Dropbox/Zotero/Wasserstein_Lazar/2016/Wasserstein and Lazar - 2016 - The ASA Statement on p-Values Context, Process, a.pdf;/Users/andrew/Zotero/storage/HEPS2L8Z/00031305.2016.html},
  number = {2}
}


