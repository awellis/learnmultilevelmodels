{
  "articles": [
    {
      "path": "about.html",
      "title": "About this blog",
      "description": "Some additional details about the blog",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-06-04T07:39:04+02:00"
    },
    {
      "path": "assignment-1.html",
      "title": "Assignment 1",
      "description": "Estimating means and standard deviations\n",
      "author": [
        {
          "name": {},
          "url": "https://github.com/awellis"
        }
      ],
      "date": "05-24-2021",
      "contents": "\n\nContents\nGenerate data\n\nBefore completing this assignment, please read the brms walkthrough.\nWe fitted a gaussian model to some simple generated data. However, in the model we assumed that both groups had the same standard deviations. This assumption is not really necessary, however, as we can easily fit a model in which we allow both mean and standard deviation to vary between groups.\n\n\nlibrary(tidyverse)\nlibrary(brms)\n\ntheme_set(theme_grey(base_size = 14) +\n            theme(panel.grid = element_blank()))\n\n\n\nGenerate data\n\n\nN <- 10\n\nmean_a <- 20.0\nsigma_a <- 2.0\n\nmean_b <- 16.0\nsigma_b <- 1.5\n\nset.seed(12)\nd <- tibble(A = rnorm(N, mean_a, sigma_a),\n            B = rnorm(N, mean_b, sigma_b))\nd <- d |>\n  pivot_longer(everything(), names_to = \"group\",\n                values_to = \"score\") |> \n  mutate(group = as_factor(group)) |> \n  arrange(group)\n\n\n\nbrms allows us easily fit both the main parameter (in this case the mean), as well as further distributional parameters. We simply need to wrap the formula in the bf() function. Therefore, instead of the formula score ~ group we can use this bf(score ~ group, sigma ~ group).\n\n\nscore ~ group\n\n\n\nCan be replaced with:\n\n\nbf(score ~ group, sigma ~ group)\n\n\n\n\nPerform the steps described in the walkthrough, but this time for the model that allows both \\(\\mu\\) and \\(\\sigma\\) to vary. Does this model perform well? Are you able to recover the true parameter values?\nNOTE: The standard deviation must be positive; therefore we are predicting log(sigma) with our linear predictor. To recover the parameter on the original scale, you need to use the inverse function, which is the exponential function exp().\nTry it out; if you get stuck, you can always ask questions on Zulip.\n\n\n\nShow code\n\nget_prior(bf(score ~ group, sigma ~ group),\n          data = d)\n\n\n                   prior     class   coef group resp  dpar nlpar\n                  (flat)         b                              \n                  (flat)         b groupB                       \n student_t(3, 16.9, 2.5) Intercept                              \n                  (flat)         b                   sigma      \n                  (flat)         b groupB            sigma      \n    student_t(3, 0, 2.5) Intercept                   sigma      \n bound       source\n            default\n       (vectorized)\n            default\n       (vectorized)\n       (vectorized)\n            default\n\n\n\nShow code\n\nm3 <- brm(bf(score ~ group, sigma ~ group),\n          prior = prior(normal(0, 4), class = b),\n          data = d, \n          file = \"models/m3\")\n\n\n\n\n\nShow code\n\nconditional_effects(m3)\n\n\n\n\n\n\nShow code\n\npp_check(m3)\n\n\n\n\n\n\nShow code\n\npp_check(m3, type = \"dens_overlay_grouped\", group = \"group\")\n\n\n\n\n\n\n\n",
      "last_modified": "2021-06-04T07:39:12+02:00"
    },
    {
      "path": "assignment-2.html",
      "title": "Assignment 2",
      "description": "Parameter estimation in clustered data\n",
      "author": [
        {
          "name": {},
          "url": "https://github.com/awellis"
        }
      ],
      "date": "05-28-2021",
      "contents": "\n\nContents\nExercise 1\nExercise 2\nExercise 3\n\n\n\nlibrary(tidyverse)\nlibrary(brms)\n\ntheme_set(theme_grey(base_size = 14) +\n            theme(panel.grid = element_blank()))\n\n\n\nExercise 1\nUse the simulate_treament() function to generate data. Vary the parameter settings, and then attempt to recover the known parameters using brm().\nExercise 2\nUse the simulate_treament() function to generate data. Give your data to a colleague and they will give you theirs in return. Try to recover the (unknown to you) parameters using brm().\nExercise 3\nLoad the dataset sleepstudy from the lme4 package. These are data from a sleep deprivation study, in which the average reaction time per day (in milliseconds) was recorded.\nHave a look at the data, and then try to estimate the effect of day on reaction time.\n\nIt’s always a good idea to plot the data before you do anything else.\n\n\n\nlibrary(lme4)\nglimpse(sleepstudy)\n\n\nRows: 180\nColumns: 3\n$ Reaction <dbl> 249.5600, 258.7047, 250.8006, 321.4398, 356.8519, 4…\n$ Days     <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, …\n$ Subject  <fct> 308, 308, 308, 308, 308, 308, 308, 308, 308, 308, 3…\n\n\n\n\n",
      "last_modified": "2021-06-04T07:39:13+02:00"
    },
    {
      "path": "assignment-3.html",
      "title": "Assignment 3",
      "description": "Pupil popularity and extraversion\n",
      "author": [
        {
          "name": {},
          "url": "https://github.com/awellis"
        }
      ],
      "date": "05-28-2021",
      "contents": "\n\nContents\nDownload data\nIntercept-only model\nFirst level predictors\nSecond level predictors\nCross-level interaction\n\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(brms)\n\ntheme_set(theme_grey(base_size = 14) +\n            theme(panel.grid = element_blank()))\n\n\n\nWe’ll look at a dataset containing popularity ratings (given by classmates) and various personal characteristics of pupils in different classes. The data are available from thecompanion website of a book on multilevel analysis (“Multilevel Analysis: Techniques and Applications, Third Edition” n.d.). The code used here borrows heavily from one of the authors’ website.\nDownload data\n\n\nShow code\n\npopularity <- haven::read_sav(file = \"https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true\")\n\n\n\n\n\nShow code\n\npopularity <- popularity |> \n  select(-starts_with(\"Z\"), -Cextrav, - Ctexp, -Csex) |> \n  mutate(sex = haven::as_factor(sex),\n         pupil = as_factor(pupil),\n         class = as_factor(class))\n\npopularity\n\n\n# A tibble: 2,000 x 7\n   pupil class extrav sex    texp popular popteach\n   <fct> <fct>  <dbl> <fct> <dbl>   <dbl>    <dbl>\n 1 1     1          5 girl     24     6.3        6\n 2 2     1          7 boy      24     4.9        5\n 3 3     1          4 girl     24     5.3        6\n 4 4     1          3 girl     24     4.7        5\n 5 5     1          5 girl     24     6          6\n 6 6     1          4 boy      24     4.7        5\n 7 7     1          5 boy      24     5.9        5\n 8 8     1          4 boy      24     4.2        5\n 9 9     1          5 boy      24     5.2        5\n10 10    1          5 boy      24     3.9        3\n# … with 1,990 more rows\n\nThe variables are\n- pupil: ID   \n- class: which class are pupils in?\n- extrav: extraversion score\n- sex: girl or boy\n- texp: teacher experience\n- popular: popularity rating\n- popteach: teacher popularity\n- Zextrav: z-transformed extraversion score           \nYou want to predict pupils’ popularity using their extraversion, gender and teacher experience.\n\nIt is important to consider which the predictor variables are at. extrav and sex are level-1 predictors, which means they are variables which vary with each observation (here this means by pupils), whereas texp is a level-2 predictor—this does not vary by observation, but by class. In other words, teacher experience is an attribute of class.\n\n\nYou should center the predictor variables.\nHow many pupils are there per class?\n\n\n\nShow code\n\nglimpse(popularity)\n\n\nRows: 2,000\nColumns: 7\n$ pupil    <fct> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ class    <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ extrav   <dbl> 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, …\n$ sex      <fct> girl, boy, girl, girl, girl, boy, boy, boy, boy, bo…\n$ texp     <dbl> 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ popular  <dbl> 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5…\n$ popteach <dbl> 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, …\n\n\n\nShow code\n\npopularity <- popularity |> \n  mutate(teacher_exp = texp - mean(texp))\n\n\n\n\n\nShow code\n\npopularity |> \n  count(class)\n\n\n# A tibble: 100 x 2\n   class     n\n   <fct> <int>\n 1 1        20\n 2 2        20\n 3 3        18\n 4 4        23\n 5 5        21\n 6 6        20\n 7 7        21\n 8 8        20\n 9 9        20\n10 10       24\n# … with 90 more rows\n\n\n\nShow code\n\npopularity |> \n  group_by(class) |> \n  n_groups()\n\n\n[1] 100\n\nWe can plot the data, without taking into account the hierarchical structure.\n\n\nShow code\n\npopularity |> \n  ggplot(aes(x = extrav,\n           y = popular,\n           color = class,\n           group = class)) + \n  geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\") +\n  theme(legend.position = \"none\") +\n  scale_color_viridis_d() +\n  labs(title = \"Popularity ~ Extraversion\")\n\n\n\n\nThe goal here it estimate the average effect of extraversion on popularity. However, we assume that this effect will vary by class, and it might also depend on the pupils’ sex. Furthermore, classes may vary by how many years of experience a teacher has. We assume that this might be important.\nIntercept-only model\nStart by fitting an intercept-only model. With this we will predict\n\n\nShow code\n\nfit1 <- brm(popular ~ 1 + (1 | class),\n            data = popularity,\n            file = \"models/pop-fit1\")\n\n\n\nIn this model, we are estimating the average the average popularity over classes, as well as the deviation from this average for each class.\n\\[\n\\begin{aligned}\n  \\operatorname{popular}_{i}  &\\sim N \\left(\\alpha_{j[i]}, \\sigma^2 \\right) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for class j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\nFirst level predictors\nNow you can add some level 1 predictors, e.g. sex, extrav. You can use the update() so that you don’t have to rerun the compilation steps.\n\\[\n\\begin{aligned}\n  \\operatorname{popular}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{sex}_{\\operatorname{girl}}) + \\beta_{2}(\\operatorname{extrav}), \\sigma^2 \\right) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for class j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\nShow code\n\nfit1 |> \n  update(. ~ . + sex, \n         prior = prior(normal(0, 2), class = b),\n         newdata = popularity)\n\n\n\nThis is equivalent to\n\n\nShow code\n\nfit2 <- brm(popular ~ 1 + sex + extrav + (1|class),  \n              prior = prior(normal(0, 2), class = b),\n              data = popularity, \n            iter = 4000,\n            file = \"models/pop-fit2\") \n\n\n\nSecond level predictors\nNow add the the level-2 predictor teacher experience.\n\\[\n\\begin{aligned}\n  \\operatorname{popular}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{sex}_{\\operatorname{girl}}) + \\beta_{2}(\\operatorname{extrav}), \\sigma^2 \\right) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\gamma_{0}^{\\alpha} + \\gamma_{1}^{\\alpha}(\\operatorname{teacher\\_exp}), \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for class j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\nShow code\n\nfit3 <- fit2 |> update(. ~ . + teacher_exp,\n                       file = \"models/pop-fit3\",\n                       newdata = popularity)\n\n\n\nor equivalently\n\n\nShow code\n\nfit3 <- brm(popular ~ 1 + sex + extrav + teacher_exp + (1 | class),  \n            prior = prior(normal(0, 2), class = b),\n            data = popularity,\n            file = \"models/pop-fit3\") \n\n\n\nNow it’s time for some plots.\n\n\nShow code\n\nfit2 |> mcmc_plot()\n\n\n\n\n\n\nShow code\n\nfit3 |> mcmc_plot()\n\n\n\n\nModel comparisons\n\n\nShow code\n\nloo2 <- loo(fit2)\nloo3 <- loo(fit3)\n\n\n\n\n\nShow code\n\nloo_compare(loo2, loo3)\n\n\n     elpd_diff se_diff\nfit3  0.0       0.0   \nfit2 -1.6       2.5   \n\n\n\nShow code\n\nbayes_R2(fit2)\n\n\n    Estimate   Est.Error      Q2.5     Q97.5\nR2 0.6903738 0.006662371 0.6768908 0.7027676\n\nShow code\n\nbayes_R2(fit3)\n\n\n    Estimate  Est.Error      Q2.5     Q97.5\nR2 0.6905302 0.00680099 0.6767647 0.7035434\n\n\n\nShow code\n\nperformance::r2_bayes(fit2)\n\n\n# Bayesian R2 with Standard Error\n\n  Conditional R2: 0.691 (89% CI [0.680, 0.701])\n     Marginal R2: 0.388 (89% CI [0.372, 0.405])\n\nShow code\n\nperformance::r2_bayes(fit3)\n\n\n# Bayesian R2 with Standard Error\n\n  Conditional R2: 0.691 (89% CI [0.680, 0.701])\n     Marginal R2: 0.510 (89% CI [0.485, 0.536])\n\nCross-level interaction\nLet teacher experience interact with extraversion. This is what’s known as a cross-level interaction; extraversion is a predictor of the level 1 units (pupils), whereas teacher experience is s predictor at level 2 (classes). This can be verified by looking at the dataframe—teacher experience does not have one unique value per observation, but instead for each class.\n\\[\n\\begin{aligned}\n  \\operatorname{popular}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{sex}_{\\operatorname{girl}}) + \\beta_{2j[i]}(\\operatorname{extrav}), \\sigma^2 \\right) \\\\    \n\\left(\n  \\begin{array}{c} \n    \\begin{aligned}\n      &\\alpha_{j} \\\\\n      &\\beta_{2j}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c} \n    \\begin{aligned}\n      &\\gamma_{0}^{\\alpha} + \\gamma_{1}^{\\alpha}(\\operatorname{teacher\\_exp}) \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}(\\operatorname{teacher\\_exp})\n    \\end{aligned}\n  \\end{array}\n\\right)\n, \n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\alpha_{j}} & \\rho_{\\alpha_{j}\\beta_{2j}} \\\\ \n     \\rho_{\\beta_{2j}\\alpha_{j}} & \\sigma^2_{\\beta_{2j}}\n  \\end{array}\n\\right)\n \\right)\n    \\text{, for class j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\nShow code\n\nfit4 <- brm(popular ~ 1 + sex + extrav + teacher_exp + extrav:teacher_exp + \n                (1 | class), \n            prior = prior(normal(0, 2), class = b),\n            data  = popularity,\n            iter = 4000,\n            file_refit = \"on_change\",\n            fit = \"models/pop-fi4\")\n\n\n\n\n\nShow code\n\nconditional_effects(fit4, \"extrav:teacher_exp\")\n\n\n\n\n\nYou can attempt to decide which models fit better than others by using loo.\n\n\n\n\n“Multilevel Analysis: Techniques and Applications, Third Edition.” n.d. Routledge & CRC Press. Accessed May 29, 2021. https://www.routledge.com/Multilevel-Analysis-Techniques-and-Applications-Third-Edition/Hox-Moerbeek-Schoot/p/book/9781138121362.\n\n\n\n\n",
      "last_modified": "2021-06-04T07:40:02+02:00"
    },
    {
      "path": "assignment-4.html",
      "title": "Assignment 4",
      "description": "Effect of tDCS stimulation on episodic memory\n",
      "author": [
        {
          "name": {},
          "url": "https://github.com/awellis"
        }
      ],
      "date": "05-28-2021",
      "contents": "\n\nContents\nEffect of tDCS on memory\nLinear model\nNull model\nModel comparison using loo\nModel comparison via Bayes factor\nControl for age\n\n\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(brms)\n\ntheme_set(theme_grey(base_size = 14) +\n            theme(panel.grid = element_blank()))\n\n\n\nEffect of tDCS on memory\nYou are going to analyze data from a study in which 34 elderly subjects were given anodal (activating) tDCS over their left tempero-parietal junction (TPJ). Subjects were instructed to learn association between images and pseudo-words (data were inspired by Antonenko et al. (2019)).\nEpisodic memory is measured by percentage of correctly recalled word-image pairings. We also have response times for correct decisions.\nEach subject was tested 5 times in the TPJ stimulation condition, and a further 5 times in a sham stimulation condition.\nThe variables in the dataset are:\nsubject: subject ID\nstimulation: TPJ or sham (control)\nblock: block\nage: age\ncorrect: accuracy per block\nrt: mean RTs for correct responses\nYou are mainly interested in whether recall ability is better during the TPJ stimulation condition than during sham.\n\n\nShow code\n\nd <- read_csv(\"https://raw.githubusercontent.com/kogpsy/neuroscicomplab/main/data/tdcs-tpj.csv\")\n\n\n\n\n\nShow code\n\nglimpse(d)\n\n\nRows: 340\nColumns: 6\n$ subject     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, …\n$ stimulation <chr> \"control\", \"control\", \"control\", \"control\", \"con…\n$ block       <dbl> 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, …\n$ age         <dbl> 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 67, 67, …\n$ correct     <dbl> 0.8, 0.4, 0.4, 0.4, 0.6, 0.4, 0.4, 0.6, 0.6, 0.8…\n$ rt          <dbl> 1.650923, 1.537246, 1.235620, 1.671189, 1.408333…\n\n\n\nShow code\n\nd <- d %>% \n  mutate(across(c(subject, stimulation, block), ~as_factor(.))) %>% \n  drop_na()\n\n\n\n\nspecify a linear model\nspecify a null model\ncompare models using approximate leave-one-out cross-validation\nestimate a Bayes factor for the effect of TPJ stimulation\nusing the Savage-Dickey density ratio test\nusing the bridge sampling method\n\ncontrol for subjects’ age\n\nLinear model\n\n\nShow code\n\nfit1 <- brm(correct ~ stimulation + (stimulation| subject),\n            prior = prior(normal(0, 1), class = b),\n            data = d,\n            file = \"models/ass4-1\")\n\n\n\n\n\nShow code\n\nsummary(fit1)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: correct ~ stimulation + (stimulation | subject) \n   Data: d (Number of observations: 315) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nGroup-Level Effects: \n~subject (Number of levels: 34) \n                              Estimate Est.Error l-95% CI u-95% CI\nsd(Intercept)                     0.04      0.02     0.00     0.08\nsd(stimulationTPJ)                0.03      0.02     0.00     0.08\ncor(Intercept,stimulationTPJ)    -0.12      0.56    -0.96     0.93\n                              Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                 1.00     1063     1691\nsd(stimulationTPJ)            1.00     1773     1704\ncor(Intercept,stimulationTPJ) 1.00     2638     2093\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept          0.43      0.02     0.39     0.46 1.00     4370\nstimulationTPJ     0.10      0.02     0.05     0.14 1.00     5662\n               Tail_ESS\nIntercept          2819\nstimulationTPJ     2553\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.19      0.01     0.18     0.21 1.00     4488     2904\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nShow code\n\nmcmc_plot(fit1, \"b_\", type = \"areas\")\n\n\n\n\nNull model\n\n\nShow code\n\nfit2 <- brm(correct ~ 1 + (stimulation| subject),\n            data = d,\n            file = \"models/ass4-2\")\n\n\n\nModel comparison using loo\n\n\nShow code\n\nloo_fit_1 <- loo(fit1)\nloo_fit_1\n\n\n\nComputed from 4000 by 315 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo     62.3 11.3\np_loo        14.4  1.1\nlooic      -124.7 22.6\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\n\nShow code\n\nloo_fit_2 <- loo(fit2)\nloo_fit_2\n\n\n\nComputed from 4000 by 315 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo     54.3 10.8\np_loo        17.0  1.3\nlooic      -108.5 21.7\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     313   99.4%   1428      \n (0.5, 0.7]   (ok)         2    0.6%   1509      \n   (0.7, 1]   (bad)        0    0.0%   <NA>      \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\n\nShow code\n\nloo_compare(loo_fit_1, loo_fit_2)\n\n\n     elpd_diff se_diff\nfit1  0.0       0.0   \nfit2 -8.1       3.7   \n\nModel comparison via Bayes factor\n\n\nShow code\n\nfit1_bf <- brm(correct ~ stimulation + (stimulation| subject),\n            prior = prior(normal(0, 1), class = b),\n            data = d,\n            iter = 1e4,\n            save_pars = save_pars(all = TRUE),\n            file = \"models/ass4-bf_1\")\n\n\n\n\n\nShow code\n\nfit2_bf <- brm(correct ~ 1 + (stimulation| subject),\n            prior = prior(normal(0, 1), class = b),\n            data = d,\n            iter = 1e4,\n            save_pars = save_pars(all = TRUE),\n            file = \"models/ass4-bf_2\")\n\n\n\n\n\nShow code\n\nloglik_1 <- bridge_sampler(fit1_bf, silent = TRUE)\nloglik_2 <- bridge_sampler(fit2_bf, silent = TRUE)\n\n\n\n\n\nShow code\n\nBF_10 <- bayes_factor(loglik_1, loglik_2)\nBF_10\n\n\nEstimated Bayes factor in favor of x1 over x2: 116.01251\n\nControl for age\n\n\nShow code\n\nfit3 <- brm(correct ~ stimulation + age + (stimulation | subject),\n            prior = prior(normal(0, 1), class = b),\n            data = d,\n            file = \"models/ass4-3\")\n\n\n\n\n\nShow code\n\nfit3\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: correct ~ stimulation + age + (stimulation | subject) \n   Data: d (Number of observations: 315) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nGroup-Level Effects: \n~subject (Number of levels: 34) \n                              Estimate Est.Error l-95% CI u-95% CI\nsd(Intercept)                     0.03      0.02     0.00     0.07\nsd(stimulationTPJ)                0.03      0.02     0.00     0.08\ncor(Intercept,stimulationTPJ)    -0.07      0.56    -0.95     0.92\n                              Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                 1.00     1066     1325\nsd(stimulationTPJ)            1.00     1476     2197\ncor(Intercept,stimulationTPJ) 1.00     2435     1959\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept          0.16      0.25    -0.33     0.64 1.00     4074\nstimulationTPJ     0.10      0.02     0.05     0.14 1.00     5234\nage                0.00      0.00    -0.00     0.01 1.00     4075\n               Tail_ESS\nIntercept          2701\nstimulationTPJ     2720\nage                2688\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.19      0.01     0.18     0.21 1.00     3919     2810\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nShow code\n\nloo_fit_3 <- loo(fit3)\n\n\n\n\n\nShow code\n\nloo_compare(loo_fit_1, loo_fit_3)\n\n\n     elpd_diff se_diff\nfit1  0.0       0.0   \nfit3 -0.4       1.0   \n\n\n\nShow code\n\nfit3_bf <- brm(correct ~ stimulation + age + (stimulation| subject),\n            prior = prior(normal(0, 1), class = b),\n            data = d,\n            iter = 1e4,\n            save_pars = save_pars(all = TRUE),\n            file = \"models/ass4-bf_3\")\n\n\n\n\n\nShow code\n\nloglik_3 <- bridge_sampler(fit3_bf, silent = TRUE)\n\n\n\n\n\nShow code\n\nBF_01 <- bayes_factor(loglik_3, loglik_1)\nBF_01\n\n\nEstimated Bayes factor in favor of x1 over x2: 0.00668\n\n\n\n\nAntonenko, Daria, Dayana Hayek, Justus Netzband, Ulrike Grittner, and Agnes Flöel. 2019. “tDCS-Induced Episodic Memory Enhancement and Its Association with Functional Network Coupling in Older Adults.” Scientific Reports 9 (1, 1): 2273. https://doi.org/10.1038/s41598-019-38630-7.\n\n\n\n\n",
      "last_modified": "2021-06-04T07:40:18+02:00"
    },
    {
      "path": "index.html",
      "title": "Learn multilevel models workshop",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-06-04T07:40:18+02:00"
    },
    {
      "path": "intro.html",
      "title": "Introduction",
      "description": "Bayesian multilevel modelling workshop 2021\n",
      "author": [
        {
          "name": {},
          "url": "https://github.com/awellis"
        }
      ],
      "date": "05-21-2021",
      "contents": "\n\nContents\nWhat is this workshop about?\nOutline\nFriday, May 21\nFriday, May 28\nSaturday, May 29\nFriday, June 4\n\nPrerequisites\nSoftware\nAssignments\nZulip\n\nWhat is this workshop about?\nThis workshop will focus on hierarchical (multilevel) regression models from various angles. While reading your expectations and questions regarding this workshop, it became clear that all of your points can be approached within the same framework, i.e. Bayesian hierarchical generalized regression models. We will spend a lot of time learning to understand and specify these types of models. However, we will focus mainly on learning how to implement and work with these models, rather than spending time looking at the formal math. Here, I will follow the approach taken by McElreath (2020), who emphasises that even mathematicians may have trouble understanding something until they see a working algorithm.\nTherefore, this workshop will be very hands-on. Everything we do will be illustrated with working code examples, and you are encouraged to try every single line of code for yourself.\nOutline\nFriday, May 21\nWe will start with a general introduction to Bayesian inference, followed by an intro to the programming language Stan, and the R packages rstan and brms. Since we will be working almost exclusively with brms therafter, it is important to spend a bit of time here.\nWe will dive straight into Bayesian inference here, without spending too much time on frequentist methods and the differences between the two approaches (we will focus on this mode when we get to model comparisons).\nWe will then explore how models can be implemented as general (or generalized) linear models, and we will introduce multilevel models as a natural way of modelling reapeated measurements.\nFriday, May 28\nSaturday, May 29\nFriday, June 4\nPrerequisites\nBasic knowledge of regression models and R is a necessity. I strongly recommend that you prepare for the workshop by working through this online script: https://methodenlehre.github.io/intro-to-rstats. Previous exposure to multilevel models and longitudinal models would be helpful, but is not strictly necessary. Knowledge of Bayesian statistics is not required.\nSoftware\nWe will be using R and RStudio, as well a variety of R packages. It is advisable to ensure that you have a working R installation before the workshop starts, and that you install the two R packages rstan and brms. Detailed instructions for installing these packages on all platforms can be found at https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started and https://paul-buerkner.github.io/brms.\nAssignments\nWe will focus on learning new topics during the morning sessions, and participants should work through the assignments during the afternoon sessions. Participants are also encouraged to bring their own datasets.\nZulip\nThe Zulip chat server will be our communicatin platform for this workshop. We will use this for questions, assignments, troubleshooting, etc. Zulip can be used for synchronous or asynchronous chats, and has very good threading capabilities. Zulip is also pretty easy to use, and uses Markdown for message formatting. This means that you can use Markdown to format code and equations.\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan, 2nd Edition. 2nd ed. CRC Press. http://xcelab.net/rm/statistical-rethinking/.\n\n\n\n\n",
      "last_modified": "2021-06-04T07:40:19+02:00"
    },
    {
      "path": "popularity.html",
      "title": "Assignment 3",
      "description": "Pupil popularity\n",
      "author": [
        {
          "name": {},
          "url": "https://github.com/awellis"
        }
      ],
      "date": "05-28-2021",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2021-06-04T07:40:20+02:00"
    },
    {
      "path": "solution-3.html",
      "title": "Assignment 3",
      "description": "Pupil popularity and extraversion\n",
      "author": [
        {
          "name": {},
          "url": "https://github.com/awellis"
        }
      ],
      "date": "05-28-2021",
      "contents": "\n\nContents\nDownload data\nIntercept-only model\nFirst level predictors\nSecond level predictors\nCross-level interaction\n\n\n\nShow code\n\nlibrary(tidyverse)\nlibrary(brms)\n\ntheme_set(theme_grey(base_size = 14) +\n            theme(panel.grid = element_blank()))\n\n\n\nWe’ll look at a dataset containing popularity ratings (given by classmates) and various personal characteristics of pupils in different classes. The data are available from thecompanion website of a book on multilevel analysis (“Multilevel Analysis: Techniques and Applications, Third Edition” n.d.). The code used here borrows heavily from one of the author’s website.\nDownload data\n\n\nShow code\n\npopularity <- haven::read_sav(file = \"https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true\")\n\n\n\n\n\nShow code\n\npopularity <- popularity |> \n  select(-starts_with(\"Z\"), -Cextrav, - Ctexp, -Csex) |> \n  mutate(sex = haven::as_factor(sex),\n         pupil = as_factor(pupil),\n         class = as_factor(class))\n\npopularity\n\n\n# A tibble: 2,000 x 7\n   pupil class extrav sex    texp popular popteach\n   <fct> <fct>  <dbl> <fct> <dbl>   <dbl>    <dbl>\n 1 1     1          5 girl     24     6.3        6\n 2 2     1          7 boy      24     4.9        5\n 3 3     1          4 girl     24     5.3        6\n 4 4     1          3 girl     24     4.7        5\n 5 5     1          5 girl     24     6          6\n 6 6     1          4 boy      24     4.7        5\n 7 7     1          5 boy      24     5.9        5\n 8 8     1          4 boy      24     4.2        5\n 9 9     1          5 boy      24     5.2        5\n10 10    1          5 boy      24     3.9        3\n# … with 1,990 more rows\n\nThe variables are\n- pupil: ID   \n- class: which class are pupils in?\n- extrav: extraversion score\n- sex: girl or boy\n- texp: teacher experience\n- popular: popularity rating\n- popteach: teacher popularity\n- Zextrav: z-transformed extraversion score           \nYou want to predict pupils’ popularity using their extraversion, gender and teacher experience.\n\nIt is important to consider which the predictor variables are at. extrav and sex are level-1 predictors, which means they are variables which vary with each observation (here this means by pupils), whereas texp is a level-2 predictor—this does not vary by observation, but by class. In other words, teacher experience is an attribute of class.\n\n\nYou should center the predictor variables.\nHow many pupils are there per class?\n\n\n\nShow code\n\nglimpse(popularity)\n\n\nRows: 2,000\nColumns: 7\n$ pupil    <fct> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ class    <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ extrav   <dbl> 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, …\n$ sex      <fct> girl, boy, girl, girl, girl, boy, boy, boy, boy, bo…\n$ texp     <dbl> 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ popular  <dbl> 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5…\n$ popteach <dbl> 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, …\n\n\n\nShow code\n\npopularity <- popularity |> \n  mutate(teacher_exp = texp - mean(texp))\n\n\n\n\n\nShow code\n\npopularity |> \n  count(class)\n\n\n# A tibble: 100 x 2\n   class     n\n   <fct> <int>\n 1 1        20\n 2 2        20\n 3 3        18\n 4 4        23\n 5 5        21\n 6 6        20\n 7 7        21\n 8 8        20\n 9 9        20\n10 10       24\n# … with 90 more rows\n\n\n\nShow code\n\npopularity |> \n  group_by(class) |> \n  n_groups()\n\n\n[1] 100\n\nWe can plot the data, without taking into account the hierarchical structure.\n\n\nShow code\n\npopularity |> \n  ggplot(aes(x = extrav,\n           y = popular,\n           color = class,\n           group = class)) + \n  geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\") +\n  theme(legend.position = \"none\") +\n  scale_color_viridis_d() +\n  labs(title = \"Popularity ~ Extraversion\")\n\n\n\n\nThe goal here it estimate the average effect of extraversion on popularity. However, we assume that this effect will vary by class, and it might also depend on the pupils’ sex. Furthermore, classes may vary by how many years of experience a teacher has. We assume that this might be important.\nIntercept-only model\nStart by fitting an intercept-only model. With this we will predict\n\n\nShow code\n\nfit1 <- brm(popular ~ 1 + (1 | class),\n            data = popularity,\n            file = \"models/pop-fit1\")\n\n\n\nIn this model, we are estimating the average the average popularity over classes, as well as the deviation from this average for each class.\n\\[\n\\begin{aligned}\n  \\operatorname{popular}_{i}  &\\sim N \\left(\\alpha_{j[i]}, \\sigma^2 \\right) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for class j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\nFirst level predictors\nNow you can add some level 1 predictors, e.g. sex, extrav. You can use the update() so that you don’t have to rerun the compilation steps.\n\\[\n\\begin{aligned}\n  \\operatorname{popular}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{sex}_{\\operatorname{girl}}) + \\beta_{2}(\\operatorname{extrav}), \\sigma^2 \\right) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for class j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\nShow code\n\nfit1 |> \n  update(. ~ . + sex, \n         prior = prior(normal(0, 2), class = b),\n         newdata = popularity)\n\n\n\nThis is equivalent to\n\n\nShow code\n\nfit2 <- brm(popular ~ 1 + sex + extrav + (1|class),  \n              prior = prior(normal(0, 2), class = b),\n              data = popularity, \n            iter = 4000,\n            file = \"models/pop-fit2\") \n\n\n\nSecond level predictors\nNow add the the level-2 predictor teacher experience.\n\\[\n\\begin{aligned}\n  \\operatorname{popular}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{sex}_{\\operatorname{girl}}) + \\beta_{2}(\\operatorname{extrav}), \\sigma^2 \\right) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\gamma_{0}^{\\alpha} + \\gamma_{1}^{\\alpha}(\\operatorname{teacher\\_exp}), \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for class j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\nShow code\n\nfit3 <- fit2 |> update(. ~ . + teacher_exp,\n                       file = \"models/pop-fit3\",\n                       newdata = popularity)\n\n\n\nor equivalently\n\n\nShow code\n\nfit3 <- brm(popular ~ 1 + sex + extrav + teacher_exp + (1 | class),  \n            prior = prior(normal(0, 2), class = b),\n            data = popularity,\n            file = \"models/pop-fit3\") \n\n\n\nNow it’s time for some plot.\n\n\nShow code\n\nfit2 |> mcmc_plot()\n\n\n\n\n\n\nShow code\n\nfit3 |> mcmc_plot()\n\n\n\n\nModel comparisons\n\n\nShow code\n\nloo2 <- loo(fit2)\nloo3 <- loo(fit3)\n\n\n\n\n\nShow code\n\nloo_compare(loo2, loo3)\n\n\n     elpd_diff se_diff\nfit3  0.0       0.0   \nfit2 -1.6       2.5   \n\n\n\nShow code\n\nbayes_R2(fit2)\n\n\n    Estimate   Est.Error      Q2.5     Q97.5\nR2 0.6903738 0.006662371 0.6768908 0.7027676\n\nShow code\n\nbayes_R2(fit3)\n\n\n    Estimate  Est.Error      Q2.5     Q97.5\nR2 0.6905302 0.00680099 0.6767647 0.7035434\n\n\n\nShow code\n\nperformance::r2_bayes(fit2)\n\n\n# Bayesian R2 with Standard Error\n\n  Conditional R2: 0.691 (89% CI [0.680, 0.701])\n     Marginal R2: 0.388 (89% CI [0.372, 0.405])\n\nShow code\n\nperformance::r2_bayes(fit3)\n\n\n# Bayesian R2 with Standard Error\n\n  Conditional R2: 0.691 (89% CI [0.680, 0.701])\n     Marginal R2: 0.510 (89% CI [0.485, 0.536])\n\nCross-level interaction\nLet teacher experience interact with extraversion. This is what’s known as a cross-level interaction; extraversion is a predictor of the level 1 units (pupils), whereas teacher experience is s predictor at level 2 (classes). This can be verified by looking at the dataframe—teacher experience does not have one unique value per observation, but instead for each class.\n\\[\n\\begin{aligned}\n  \\operatorname{popular}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{sex}_{\\operatorname{girl}}) + \\beta_{2j[i]}(\\operatorname{extrav}), \\sigma^2 \\right) \\\\    \n\\left(\n  \\begin{array}{c} \n    \\begin{aligned}\n      &\\alpha_{j} \\\\\n      &\\beta_{2j}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c} \n    \\begin{aligned}\n      &\\gamma_{0}^{\\alpha} + \\gamma_{1}^{\\alpha}(\\operatorname{texp}) \\\\\n      &\\gamma^{\\beta_{2}}_{0} + \\gamma^{\\beta_{2}}_{1}(\\operatorname{texp})\n    \\end{aligned}\n  \\end{array}\n\\right)\n, \n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\alpha_{j}} & \\rho_{\\alpha_{j}\\beta_{2j}} \\\\ \n     \\rho_{\\beta_{2j}\\alpha_{j}} & \\sigma^2_{\\beta_{2j}}\n  \\end{array}\n\\right)\n \\right)\n    \\text{, for class j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\nShow code\n\nmodel5 <- brm(popular ~ 1 + sex + extrav + texp + extrav:texp + \n                (1 + extrav | class), \n              prior = prior(normal(0, 2), class = b),\n              data  = popularity)\n\n\n\n\nYou can attempt to decide which models fit better than others by doing posterior predictive checks.\n\n\n\n\n“Multilevel Analysis: Techniques and Applications, Third Edition.” n.d. Routledge & CRC Press. Accessed May 29, 2021. https://www.routledge.com/Multilevel-Analysis-Techniques-and-Applications-Third-Edition/Hox-Moerbeek-Schoot/p/book/9781138121362.\n\n\n\n\n",
      "last_modified": "2021-06-04T07:40:41+02:00"
    },
    {
      "path": "test.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\n\nShow code\n\nknitr::opts_chunk$set()\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\nShow code\n\nc(\"dogs\", \"cats\", \"rats\") |>\n      {\\(x) grepl(\"at\", x)}()\n\n\n[1] FALSE  TRUE  TRUE\n\n\n\n\n",
      "last_modified": "2021-06-04T07:40:41+02:00"
    },
    {
      "path": "topics.html",
      "title": "Topics",
      "description": "Bayesian multilevel modelling workshop 2021\n",
      "author": [
        {
          "name": {},
          "url": "https://github.com/awellis"
        }
      ],
      "date": "05-21-2021",
      "contents": "\n\nContents\nTopics\nPlanned contents\nYour expectations/questions\n\n\nTopics\nPlanned contents\nThis workshop is designed to provide a practical introduction to basic and advanced multilevel models. Participants will learn how to fit models using both maximum likelihood and Bayesian methods, although the focus will be on Bayesian parameter estimation and model comparison. We will start with a short introduction to multilevel modelling and to Bayesian statistics in general, followed by an introduction to Stan, a probabilistic programming language for fitting Bayesian models. We will then learn how to use the R package brms, which provides a user-friendly interface to Stan. The package supports a wide range of response distributions and modelling options, and allows us to fit multilevel generalized linear models. Depending on participants’ wishes, we will take a closer look at modelling various types of data, such as choices, response times, ordinal or longitudinal data.\nSpecific topics include:\nBayesian inference: an introduction\nBayesian parameter estimation\nModel comparison & hypothesis testing\nBayes factors\nOut-of-sample predictive accuracy (LOO)\n\nSpecifying multilevel generalized linear models\nUnderstanding statistical models through data simulation\nA principled Bayesian workflow for data analysis\nYour expectations/questions\nSince most of you expressed an interest in Bayesian statistics, we will mostly multilevel generalized regression models from this perspective.\nSpecific topics\nadvantages of Bayesian over frequentist statistics\nhow to select priors\npreparing data\nbinary (choice) data\nresponse times\nrepeated measures and other hierarchical (multilevel) designs, inlcuding longitudinal data\ncrossed random effects, e.g. lexical decision tasks\ncategorical variables\nmoderation / mediation\nSEM (structural equation models)\nmodel comparison / hypothesis testing in mixed effects models\ndyadic data\nnonlinear regression\n\n\n\n",
      "last_modified": "2021-06-04T07:40:42+02:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
