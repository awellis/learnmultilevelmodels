[
  {
    "path": "posts/2021-05-24-walkthrough-brms/",
    "title": "An Introduction to brms",
    "description": "Using `brms` for parameter estimation: A walkthrough",
    "author": [
      {
        "name": "Andrew Ellis",
        "url": "https://github.com/awellis"
      }
    ],
    "date": "2021-05-23",
    "categories": [
      "wokflow",
      "data analysis"
    ],
    "contents": "\n\nContents\nGenerate data\nProbabilistic model\nLinear model\nPrior distributions\n\nPrior predictive distribution\nPrior predictive checks\n\nPosterior inference\nParameter estimates\nConditional means\nPosterior predictive check\nAll in one\n\n\n\nI am using the native pipe operator, which is new to R 4.10. This pipe operator is written as a | followed by a >. In this document, the operator is printed as |>, due to the fact that I am using font ligatures. If the pipe doesnâ€™t work for you, simply replace it with the older pipe %>%.\n\n\n\nlibrary(tidyverse)\nlibrary(brms)\n\ntheme_set(theme_grey(base_size = 14) +\n            theme(panel.grid = element_blank()))\n\n\n\nIn this post, Iâ€™ll show how to use brms to infer the means of two independent normally distributed samples. Iâ€™ll try to follow the steps illustrated in the previous post on a principled Bayesian workflow.\nGenerate data\nFirst, weâ€™ll generate two independent normally distributed samples. These will correspond to two levels of a grouping variable, so letâ€™s call them group A and group B.\nGroup A will have a mean \\(\\mu_A = 20\\) and a standard deviation \\(\\sigma_A = 2\\), whereas group B have have the parameters \\(\\mu_B = 16\\) and \\(\\sigma_B = 1.5\\).\n\n\nmean_a <- 20.0\nsigma_a <- 2.0\n\nmean_b <- 16.0\nsigma_b <- 1.5\n\ntrue_params <- tribble(~Group, ~Mean, ~SD,\n                  \"A\", mean_a, sigma_a,\n                  \"B\", mean_b, sigma_b) \ntrue_params |> \n    kableExtra::kbl() |> \n    kableExtra::kable_paper(\"hover\", full_width = T)\n\n\n\nGroup\n\n\nMean\n\n\nSD\n\n\nA\n\n\n20\n\n\n2.0\n\n\nB\n\n\n16\n\n\n1.5\n\n\nWe now draw 10 observations for each group.\n\n\nN <- 10\n\nset.seed(12)\nd <- tibble(A = rnorm(N, mean_a, sigma_a),\n            B = rnorm(N, mean_b, sigma_b))\nd <- d |>\n  pivot_longer(everything(), names_to = \"group\",\n                values_to = \"score\") |> \n  mutate(group = as_factor(group)) |> \n  arrange(group)\n\n\n\nSince we know the true values that generated the data, we know whether we will be able to successfully recover them. Of course, the sample means and standard deviations will differ slightly from the true values.\n\n\nfuns <- list(mean = mean, median = median, sd = sd)\nd |>\n  group_by(group) |>\n  summarise(across(everything(), funs, .names = \"{.fn}\"))\n\n\n# A tibble: 2 x 4\n  group  mean median    sd\n  <fct> <dbl>  <dbl> <dbl>\n1 A      19.1   19.1  2.00\n2 B      15.7   15.7  1.11\n\nProbabilistic model\nWe assume the data are conditionally normally distributed\n\\[ y_i \\sim \\mathcal{N}(\\mu_{[j]}, \\sigma_{[j]}) \\] \\[ \\text{for J = 1, 2} \\]\nWe will initially assume that the two groups have equal standard deviations (SD), so that we need only estimate one common SD parameter. We therefore need to estimate three parameters in total, \\(\\mu_a, \\mu_b, \\sigma\\) (you can allow both mean and standard deviation to vary in assignment 1).\nLinear model\nUsing a linear model, we have several possibilities for choosing our contrast coding. We will use treatment coding, in which we choose one of the groups as reference category. This will be represented by the intercept. The other group will not be estimated directly. Instead, the second parameter will represent the difference between this group and the reference category.\nWe can check the levels of the grouping variable. The first levels will be chose as the reference group.\n\n\nlevels(d$group)\n\n\n[1] \"A\" \"B\"\n\nAnother possibility is to omit the intercept, and then just estimate both group means independently.\n\nFor parameter estimation, the difference between the two approaches will affect the choice of priors, but not the conclusions we can draw. Once we have the posterior distributions, we can compute any quantity of interest directly from the posterior samples. The choice of contrast coding will become important later, when we look at model comparisons.\nFor the first approach, we use the R formula\n\n\nscore ~ 1 + group\n\n\nscore ~ 1 + group\n\n# or eqivalently\nscore ~ group\n\n\nscore ~ group\n\nFor the second parameterization, we write\n\n\nscore ~ 0 + group\n\n\nscore ~ 0 + group\n\n# or eqivalently\nscore ~ group - 1\n\n\nscore ~ group - 1\n\nPrior distributions\nWe can check which for which parameters we need to set priors, and what the default priors are, using the get_prior() function.\n\n\nget_prior(score ~ group, data = d)\n\n\n                   prior     class   coef group resp dpar nlpar bound\n                  (flat)         b                                   \n                  (flat)         b groupB                            \n student_t(3, 16.9, 2.5) Intercept                                   \n    student_t(3, 0, 2.5)     sigma                                   \n       source\n      default\n (vectorized)\n      default\n      default\n\nThe output doesnâ€™t look very appealing, so we can show just the first four columns:\n\n\nget_prior(score ~ group, data = d) |> \n  tibble() |> select(1:4)\n\n\n# A tibble: 4 x 4\n  prior                     class     coef     group\n  <chr>                     <chr>     <chr>    <chr>\n1 \"\"                        b         \"\"       \"\"   \n2 \"\"                        b         \"groupB\" \"\"   \n3 \"student_t(3, 16.9, 2.5)\" Intercept \"\"       \"\"   \n4 \"student_t(3, 0, 2.5)\"    sigma     \"\"       \"\"   \n\nThe three parameters are groupB, represents the difference between group B and the reference category, Intercept, which represents group A, and sigma, the common standard deviation.\nBoth Intercept and sigma are given Student-t priors. The first parameter of this distribution can be considered as a â€œnormalityâ€ parameterâ€”the higher this is, the more normal the distribution looks. The prior on the intercept has a mean of 16.9, which is based on the median of the response variable (median(d$score)) and a standard devation of 2.5. The default priors are guesses to ensure that the posterior is in the raight range, while making it unlikely that the prior biases the inferences.\nSomething that is not apparent is that the prior on sigma is actually a folded Student-t distributionâ€”this means that the distribution is folded in half, because the parameter sigma is constrained to be positive (a standard deviation must \\(>0\\).\nThe prior on the groupB parameter is flat. This is basically never a good ideaâ€”you should always choose your own prior, instead of using the default flat prior.\n\nThe same goes for the other parametersâ€”you should never blindly accept the defaults. Of course, having these defaults is very useful if you want to get up and running, or if you want to quickly try out a model. However, they should be considered the starting point, and should be replaced by priors based on prior predictive checks.\nFor the second parameterization, we get\n\n\nget_prior(score ~ 0 + group, data = d)\n\n\n                prior class   coef group resp dpar nlpar bound\n               (flat)     b                                   \n               (flat)     b groupA                            \n               (flat)     b groupB                            \n student_t(3, 0, 2.5) sigma                                   \n       source\n      default\n (vectorized)\n (vectorized)\n      default\n\nHere, we get the same statndard deviation parameter, but instead of an intercept we get two parameters, one for each level of the grouping variable. Both have flat priors.\n\nOne important difference between the two is that for the second parameterization, both levels are treated in the same manner, whereas for the first approach, the reference get a prior, and the non-reference category is coded as Intercept + groupB. There the mean of group B will be estimated with more uncertainty that that of group A. While this makes sense for hypothesis testing, for estimation this is questionable. McElreath (2020) generally recommends the second approach.\n\nWe will ignore McElreathâ€™s advice for now, and estimate mean of group B as Intercept + groupB.\nSince we already know from the summary above that the difference between groups cannot tbe very large, we set a normal(0, 4) on the group difference. This expresses the belief that we are about 95% certain that the parameter will lie between \\(-8\\) and \\(8\\)\nWe can use the brms function prior() to do this.\n\n\nprior(normal(0, 4), class = b)\n\n\nb ~ normal(0, 4)\n\n\nThis may not be a particularly good prior. We are merely illustrating the wokflow here. This prior should not have much of an influence on the posterior.\nThe priors on the intercept and and group difference look like this:\n\n\nlibrary(patchwork)\n\np_intercept <- tibble(x = seq(0, 35, by = 0.01),\n       y = dstudent_t(x, 3, 16.9, 2.5)) |> \n  ggplot(aes(x, y)) + geom_line(size = 2) +\n  ylab(\"\") + xlab(\"\") +\n  ggtitle(\"Intercept\")\n\np_groupB <- tibble(x = seq(-10, 10, by = 0.01),\n       y = dnorm(x, 0, 4)) |> \n  ggplot(aes(x, y)) + geom_line(size = 2) +\n  ylab(\"\") + xlab(\"\") +\n  ggtitle(\"groupB\")\n\np_intercept + p_groupB\n\n\n\n\nPrior predictive distribution\nIn order to get the prior predictive distribution, we can first sample from the prior distributions using the sample_prior argument set to \"only\". If we do this, we are running the same model that we will later use to obtain the posterior distribution, but we are ignoring the data.\n\nThe file argument can be used to store the output of the brm() function on disk, so that it wonâ€™t be run again unless the model specification changes.\n\n\nm1_prior <- brm(score ~ group, \n               prior = prior(normal(0, 4), class = b),\n               data = d, \n               sample_prior = \"only\", \n               file = \"models/m1_prior\")\n\n\n\nThis model will do three things: 1) provide prior distributions of the parameters, 2) provide distributions of the conditional means, i.e.Â the values of the linear predictor and 3) provide samples from the prior predictive distribution.\nWe can visualize the distribution of parameter values that our model expects using the mcmc_plots() function.\n\n\nmcmc_plot(m1_prior)\n\n\n\n\nThese distributions just reflect the prior distributions, i.e.Â they are sampled from each parameterâ€™s prior distribution. It is very helpful, though, to plot the conditional means, i.e.Â the expected means conditioned on group membership.\n\nTo be precise, conditional_effects() plots the expected means of the prior predictive distribution (prior because we are only sampling from the prior in this model).\n\n\nconditional_effects(m1_prior)\n\n\n\n\nBoth groups are expected to have similar means, because that is what we expressed with our prior distribution on the group difference.\nPrior predictive checks\nWe can then add additional variance by incorporating the residual error. This can be achieved by using the posterior_predict() function and then processing the output; however, it is often far simpler to use the built-in function pp_check() (the pp stand for posterior predictive). This function cab perform a variety of posterior predictive checks; here we are simply plotting the density of the data (\\(y\\)) along with densitites obtained from generated data (\\(y_{rep}\\)).\n\nIf we sample from the posterior, then pp_check() performs posterior predictive checks. If we sample from the prior only, then pp_check() performs prior predictive checks.\n\nThis plot can give us a good idea of what kind of data our model expects, and we can compare those to the actual data obtained\n\n\npp_check(m1_prior)\n\n\n\n\nWe can also group by our grouping variable to compare the generated data separately by group.\n\n\npp_check(m1_prior, type = \"dens_overlay_grouped\", group = \"group\")\n\n\n\n\nPosterior inference\nIf we are happy with our model, we can sample from the posterior, using the same model from above, but ommitting the sample_prior argument. As above, brms generated Stan code, which is then compiled to C++. Once the model is compiled, Stan runs 4 independent Markov chains, each of which will explore the posterior distribution. The number of chains can be specified, but it is rarely necesarry to change the default setting of 4.\n\nIt is a good idea to use as many cores as possible. Modern computers have multi-core processors. This means that Stan will make use of as many cores as it can, and run the chains in parallel. This will result in a huge speed-up. You can use the argument cores = parallel::detectCores() inside brm() to set this. It advisable to set this in the R options, so that you do have to do this every time you call brm().\n\n\n\nm1 <- brm(score ~ group, \n          prior = prior(normal(0, 4), class = b),\n          data = d, \n          file = \"models/m1\")\n\n\n\nBefore we look at the parameter estimates, it essential to check that the 4 chains have converged, i.e.Â that they are sampling from the same posterior. Calling the plot() method on the fitted object will plot traceplots (on the right of the plot), which are the estimates (on the y axis) plotted against the sample number.\n\n\nplot(m1)\n\n\n\n\nAnother way of getting these is with the function mcmc_trace() from the bayesplot package.\n\n\nlibrary(bayesplot)\nmcmc_trace(m1)\n\n\n\n\nThe plots for each parameter show the 4 chains (in different shades of blue). They should not be easily visually distinguishable from each other, and should visually resemble a â€œfat hairy caterpillar.â€\nAnother method for checking convergence of the Rhat value in the output. There is one for each estimated parameter, and these values should be \\(1.0\\), or \\(> 1.05\\). The Rhat statistic measures the ratio of the average variance of samples within each chain to the variance of the pooled samples across chains. If these values are \\(1.0\\), this means that the chains have mixed well and are sampling from the same posterior.\n\n\nsummary(m1)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: score ~ group \n   Data: d (Number of observations: 20) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    18.99      0.53    17.94    20.02 1.00     3264     2501\ngroupB       -3.24      0.74    -4.68    -1.72 1.00     3224     2541\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.70      0.30     1.24     2.40 1.00     3104     2737\n\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nWe can now look at the estimated parameters. Here, we get Population-Level Effects and Family Specific Parameters. The population-level effects are our intercept and group difference, the fFamily-specific parameter is the residual standard deviation. For each parameter we are shown the mean of the posterior distribution (Estimate), the standard deviation of the posterior distribution (Est.Error) as well as two-sided 95% credible intervals(l-95% CI and u-95% CI) based on quantiles. The Bulk and Tail ESS (expected sample size) are estimates of how many independent draws would contain the same amount of information as the correlated draws of the posterior (Markov chains obtain correlated draws).\nParameter estimates\nThe three parameters are Intercept, groupB and sigma. The latter represents the stndard deviation, which according to our model is not allowd to vary between groups (our model is thus mis-specified, as we know that sigma differs between groups.) The posterior is mean is 1.7, with a 95% CI of [1.24, 2.4]. We are thus 95% certain the the standard deviation lies within that interval.\nIntercept and groupB reprsent the expected mean of the reference group, which is A in this case, and the difference between groups, respectively.\nThe intercept has a mean of 18.99, with a 95% CI of [17.94, 20.02], and the difference between groups has a mean of -3.24 with a 95% CI of [-4.68, 1.72].\n\nThe 95% CIs obtained here are credible intervals, i.e.Â summaries of the posterior distributions based on quantiles. They are NOT confidence intervals.\nThese are merely summaries of the posterior distributions. It is also very important to look at the full posterior distributions. These can be plotted with the function mcmc_plot().\n\n\nmcmc_plot(m1)\n\n\n\n\nWe can also choose which parameters to plot:\n\n\nmcmc_plot(m1, \"b_groupB\")\n\n\n\n\nConditional means\nA simple way to obtain the predicted conditional means is to use the the add_fitted_draws() from the tidybayes package.\n\n\nlibrary(tidybayes)\n\n\n\nThis requires a grid of values for which we want the conditional means. In the case we use the data_grid() function from the modelr package to create this.\n\n\ngrid <- d |> \n  modelr::data_grid(group)\n\ngrid\n\n\n# A tibble: 2 x 1\n  group\n  <fct>\n1 A    \n2 B    \n\nWe can the use add_fitted_draws() to obtain the values of the linear predictor, which in this case will be either the Intercept for group A, or Intercept + groupB for group B.\n\n\ngrid |> \n  add_fitted_draws(m1)\n\n\n# A tibble: 8,000 x 6\n# Groups:   group, .row [2]\n   group  .row .chain .iteration .draw .value\n   <fct> <int>  <int>      <int> <int>  <dbl>\n 1 A         1     NA         NA     1   19.2\n 2 A         1     NA         NA     2   19.5\n 3 A         1     NA         NA     3   19.7\n 4 A         1     NA         NA     4   18.1\n 5 A         1     NA         NA     5   18.4\n 6 A         1     NA         NA     6   18.8\n 7 A         1     NA         NA     7   19.1\n 8 A         1     NA         NA     8   19.2\n 9 A         1     NA         NA     9   18.7\n10 A         1     NA         NA    10   18.5\n# â€¦ with 7,990 more rows\n\nThese can then be plotted using the stat_pointinterval() function, which takes a .width argument to specify the width of the credible interval.\n\n\ngrid |> \n  add_fitted_draws(m1) |> \n  ggplot(aes(x = .value, y = group)) +\n  stat_pointinterval(.width = c(.66, .95))\n\n\n\n\nPosterior predictive check\nSimilarly to above, we can use pp_check(), which will now perform psterior predictive check (because we have sampled from the posterior).\n\n\npp_check(m1)\n\n\n\n\n\n\npp_check(m1, type = \"dens_overlay_grouped\", group = \"group\")\n\n\n\n\nIt is apparent the while our model can adequately represent the shape of the data, the predictions vary quite a lot, which is due to there not being enough data (this is only a toy model, after all).\nAll in one\nUsing the functions from the tidybayes package, we can plot the conditional exptected means, the posterior predictions for the data along with the actual data, all in one plot.\n\n\nfits <- grid %>%\n  add_fitted_draws(m1)\n\npreds <- grid %>%\n  add_predicted_draws(m1)\n\nd %>%\n  ggplot(aes(y = group, x = score)) +\n  stat_interval(aes(x = .prediction), data = preds) +\n  stat_pointinterval(aes(x = .value), data = fits, .width = c(.66, .95), position = position_nudge(y = -0.3)) +\n  geom_point() +\n  scale_color_brewer()\n\n\n\n\nThe blue band shows the posterior predictive density for new data (what data does our model predict?), the black dots within the blue bands show the actual data points, and the intervals underneath show the expected conditional means (values of the linear predictor).\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan, 2nd Edition. 2nd ed. CRC Press. http://xcelab.net/rm/statistical-rethinking/.\n\n\n\n\n",
    "preview": "posts/2021-05-24-walkthrough-brms/walkthrough-brms_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2021-05-26T10:19:43+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-22-principled-workflow/",
    "title": "A Principled Bayesian Workflow",
    "description": "Steps in Bayesian Data Analysis",
    "author": [
      {
        "name": "Andrew Ellis",
        "url": "https://github.com/awellis"
      }
    ],
    "date": "2021-05-22",
    "categories": [
      "wokflow",
      "data analysis"
    ],
    "contents": "\nThe following figure illustrates the steps that are ideally involved in data analysis. Whereas traditional approaches often combine inference and hypothesis testing, and consist of applying off-the-shelf methods, data analysis involves many more steps.\n\n\n\nStarting at the beginning, we gather all the knowledge we habe about our data analysis problem. This includes deciding which data to collect, which hypotheses to formulate, quantifying our prior knowledge about our parameters.\nFrom this we formulate a generative model, which is a joint probability model of the data and all parameters.\nOnce we have this, we can perform prior predictve checks. This will inform us about the data our model expects to see, and can tell if our models make sense. If the model makes nonsensical predictions, it might have to be revised.\nWe can can then simulate data from our model using a set of known parameters, and use these data in order to recover the parameters. This is an important step when using complex models, as this will tell us whether our model can recover parameters. If our model fails to do so, it needs revising.\nOnce we have gathered data, we can fit our model to the data by applying Bayes theorem. This will gives a joint posterior distribution of all the parameters.\nUsing posterior predictive checks, we can investigate whether our model can capture patterns n the data. This is an important step in deciding wtether a model is mis-specified.\nAt this point, we may decide to revise our model, thus completing the loop.\nIf we are happy that the model captures all relevant aspects of the data, we can use/interpret the posterior estimates, or we can perform model comparison.\nWe can compare several candidate models using posterior predictive checks or approximate out-of -sample predictive accuracy.\nHypothesis testing is a form of model comparison. Bayes factors can quantify relative evidence for one model over another.\n\n\n\n",
    "preview": "posts/2021-05-22-principled-workflow/../../images/principled-bayesian-workflow.png",
    "last_modified": "2021-05-22T23:54:58+02:00",
    "input_file": {},
    "preview_width": 2104,
    "preview_height": 1526
  },
  {
    "path": "posts/2021-05-21-zoom/",
    "title": "Zoom links",
    "description": "Course dates and Zoom links.",
    "author": [
      {
        "name": "Andrew Ellis",
        "url": {}
      }
    ],
    "date": "2021-05-21",
    "categories": [],
    "contents": "\nFriday mornings\nMeeting ID: 644 5562 5717\nPasscode: 879219\nMay 21, 2021 08:15\nMay 28, 2021 08:15\nJun 4, 2021 08:15\nðŸ”— Join Zoom Meeting\nSaturday morning and afternoon\nMeeting ID: 638 6457 0235\nPasscode: 888908\nMay 29, 2021 09:15\nðŸ”— Join Zoom Meeting\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-21T01:31:45+02:00",
    "input_file": {}
  }
]
